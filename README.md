# ğŸ¤– Transformer-Based Language Models (Input Talk @ Method Bits, MZES, UMannheim)
Welcome to the transformer-based Language Models (LM) talk! You can find the slides for the talk here [![Google Slides](https://img.shields.io/badge/Slides-yellow?logo=google-slides)](https://docs.google.com/presentation/d/1py8jRKvNZXCrCCwtAtwgewMHEDM7d1iKI7E4cA_sgt0/edit?usp=sharing). In this talk, we will explore the power of transformer-based LMs and how they can be used for various research tasks.

This is a talk on Transformer-based Language Models, covering a range of topics including what language models are, why we need a new architecture for them, the components that make the Transformer architecture so powerful, the differences between Transformer-based Language Models, how to train a state-of-the-art transformer model for various research tasks, and the limits and open challenges of these new types of Language Models. Related questions for this talk:
<details open>
<summary>* ğŸ¤” What are Language Models?</summary>
<br>
Well you asked for it
</details>

* ğŸš€ Why do we need a new architecture (Transformer) for Language Models?
* ğŸ”§ What components make transformer architecture so powerful?
* ğŸ¤– What are the differences between transformer-based Language Models?
* ğŸ¤“ How can we train a SotA transformer model for various research tasks?
* ğŸ¤¯ What are the limits and open challenges of these new types of Language Models?

### ğŸ“ Workshop Structure
* Introduction to Language Models
* Transformer Architecture and Components
* Differences between Transformer-Based Language Models
* Hands-on sessions to train LMs
* Limits and Open Challenges

### ğŸ¤“ Hands-on Sessions
This workshop includes three hands-on sessions using Google Colab:
* Supervised Topic Classification [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing](https://colab.research.google.com/drive/1jK_hD6XJDCEHnWj7yHyCOo8fmqjR3yx0?usp=sharing)): In this session, we will learn how to train a supervised topic classification model using a pre-trained transformer-based language model.
* Domain Adaptation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing](https://colab.research.google.com/drive/1gaZU9pTCaNyzGdZJEL7sGvq07SsSEY38?usp=sharing)): In this session, we will learn how to fine-tune an existing language model to a specific domain by leveraging transfer learning techniques.
* Zero-Shot NLI Classification [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing](https://colab.research.google.com/drive/1Big34S9VkknUnhccQR3v-Zo5-JVUhdAv?usp=sharing)): In this session, we will learn how to use an existing transformer-based language model for zero-shot NLI classification (using the model trained by [Laurer et al. 2022](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)).

### ğŸ¦œ Limitations and Open Challenges
As with any technology, Transformer-based Language Models have limitations and open challenges that need to be addressed (see [Bender et al. 2021](https://dl.acm.org/doi/10.1145/3442188.3445922)). Some of the current limitations and open challenges include:
* Environmental impact ğŸŒ³ ğŸ’¨
* Ethical considerations around the use of large-scale language models
* Ensuring the models are robust and not easily fooled ğŸ‘º by adversarial attacks
* The need for more diverse and inclusive training data ğŸ“š to reduce biases in the models
* Data privacy and security ğŸ”’
* Developing methods for more efficient training and inference with these large-scale models ğŸ‘©ğŸ¾â€ğŸ“
* Model Interpretability ğŸ”

### ğŸ“š Additional Resources
* [HuggingFace Transformers Library](https://huggingface.co)
* On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ğŸ¦œ [Bender et al. 2021](https://dl.acm.org/doi/10.1145/3442188.3445922)

### ğŸ¤– Authors
Christopher Klamm

#### ğŸ“ License
_This workshop is licensed under the MIT License._

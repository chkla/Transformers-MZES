# 🤖 Transformer-Based Language Models (Input Talk @ Method Bits, MZES, UMannheim)
Welcome to the transformer-based Language Models (LM) talk! You can find the slides for the talk here [![Google Slides](https://img.shields.io/badge/Slides-yellow?logo=google-slides)](https://docs.google.com/presentation/d/1py8jRKvNZXCrCCwtAtwgewMHEDM7d1iKI7E4cA_sgt0/edit?usp=sharing). In this talk, we will explore the power of transformer-based LMs and how they can be used for various research tasks.

This is a talk on Transformer-based Language Models, covering a range of topics including what language models are, why we need a new architecture for them, the components that make the Transformer architecture so powerful, the differences between Transformer-based Language Models, how to train a state-of-the-art transformer model for various research tasks, and the limits and open challenges of these new types of Language Models. Related questions for this talk:
<details open>
<summary>* 🤔 What are Language Models?</summary>
<br>
Well you asked for it
</details>

* 🚀 Why do we need a new architecture (Transformer) for Language Models?
* 🔧 What components make transformer architecture so powerful?
* 🤖 What are the differences between transformer-based Language Models?
* 🤓 How can we train a SotA transformer model for various research tasks?
* 🤯 What are the limits and open challenges of these new types of Language Models?

### 📝 Workshop Structure
* Introduction to Language Models
* Transformer Architecture and Components
* Differences between Transformer-Based Language Models
* Hands-on sessions to train LMs
* Limits and Open Challenges

### 🤓 Hands-on Sessions
This workshop includes three hands-on sessions using Google Colab:
* Supervised Topic Classification [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing](https://colab.research.google.com/drive/1jK_hD6XJDCEHnWj7yHyCOo8fmqjR3yx0?usp=sharing)): In this session, we will learn how to train a supervised topic classification model using a pre-trained transformer-based language model.
* Domain Adaptation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing](https://colab.research.google.com/drive/1gaZU9pTCaNyzGdZJEL7sGvq07SsSEY38?usp=sharing)): In this session, we will learn how to fine-tune an existing language model to a specific domain by leveraging transfer learning techniques.
* Zero-Shot NLI Classification [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://colab.research.google.com/drive/1K9zkPIUBPCWaVgg4duuYKirOrxAID0wG?usp=sharing](https://colab.research.google.com/drive/1Big34S9VkknUnhccQR3v-Zo5-JVUhdAv?usp=sharing)): In this session, we will learn how to use an existing transformer-based language model for zero-shot NLI classification (using the model trained by [Laurer et al. 2022](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)).

### 🦜 Limitations and Open Challenges
As with any technology, Transformer-based Language Models have limitations and open challenges that need to be addressed (see [Bender et al. 2021](https://dl.acm.org/doi/10.1145/3442188.3445922)). Some of the current limitations and open challenges include:
* Environmental impact 🌳 💨
* Ethical considerations around the use of large-scale language models
* Ensuring the models are robust and not easily fooled 👺 by adversarial attacks
* The need for more diverse and inclusive training data 📚 to reduce biases in the models
* Data privacy and security 🔒
* Developing methods for more efficient training and inference with these large-scale models 👩🏾‍🎓
* Model Interpretability 🔎

### 📚 Additional Resources
* [HuggingFace Transformers Library](https://huggingface.co)
* On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 [Bender et al. 2021](https://dl.acm.org/doi/10.1145/3442188.3445922)

### 🤖 Authors
Christopher Klamm

#### 📝 License
_This workshop is licensed under the MIT License._
